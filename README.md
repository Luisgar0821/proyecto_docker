
# ğŸ”¬ Proyecto ETL y Streaming: PredicciÃ³n de CÃ¡ncer de PulmÃ³n

Este proyecto implementa un **pipeline ETL completo** utilizando Airflow, PostgreSQL, y Apache Kafka para procesar y transmitir mÃ©tricas sobre el cÃ¡ncer de pulmÃ³n a partir de datos de salud. Incluye ademÃ¡s dashboards estÃ¡ticos en Power BI y visualizaciÃ³n **en tiempo real** con Streamlit.

---

## ğŸ“Œ Objetivo

El objetivo principal es procesar datos provenientes de un dataset masivo y una API externa, transformarlos y cargarlos en un **modelo dimensional en PostgreSQL**, y luego transmitir mÃ©tricas clave en tiempo real a travÃ©s de **Apache Kafka**, visualizÃ¡ndolas mediante un dashboard interactivo.

Este sistema busca simular un entorno real de monitoreo de salud pÃºblica relacionado con el cÃ¡ncer de pulmÃ³n, su prevalencia, hÃ¡bitos de tabaquismo, y el acceso al sistema de salud.

---

## ğŸ› ï¸ TecnologÃ­as utilizadas

- **Apache Airflow**: OrquestaciÃ³n de tareas ETL
- **PostgreSQL**: Base de datos relacional para almacenar el modelo dimensional
- **Apache Kafka**: Sistema de mensajerÃ­a para transmisiÃ³n en tiempo real
- **Python + Pandas**: Transformaciones de datos y servicios
- **Streamlit**: Dashboard en tiempo real
- **Power BI**: Dashboard exploratorio offline
- **Docker / Docker Compose**: Contenedores y despliegue

---

## ğŸ—‚ï¸ Estructura del Proyecto

```
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ etl_load_postgres_dag.py
â”‚   â”‚   â”œâ”€â”€ temp_api_data.csv
â”‚   â”‚   â”œâ”€â”€ temp_transformed.csv
â”‚   â”‚   â””â”€â”€ temp_merged.csv
â”œâ”€â”€ kafka/
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ consumer.py
â”‚   â””â”€â”€ dashboard/
â”‚       â”œâ”€â”€ realtime_dashboard.py
â”‚       â”œâ”€â”€ message_store.py
â”‚       â””â”€â”€ Dockerfile.streamlit
â”œâ”€â”€ Docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile.airflow
â”‚   â””â”€â”€ create_schema.sql
â”œâ”€â”€ Notebooks/
â”‚   â””â”€â”€ lung_cancer_EDA.ipynb
â”œâ”€â”€ Dashboard/
â”‚   â””â”€â”€ Dashboard modelo dimensional.pdf
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_transform_data.py
â””â”€â”€ requirements.txt
```

---

## ğŸ”„ Flujo ETL y de Streaming

### 1. **ExtracciÃ³n**
- Datos de un CSV local (`lung_cancer_prediction_dataset.csv`)
- Datos desde la API de la OMS sobre tabaquismo

### 2. **TransformaciÃ³n**
- Limpieza, mapeo booleano, agrupaciÃ³n por paÃ­s
- SustituciÃ³n de nulos con el valor `"any"` cuando representa casos no diagnosticados

### 3. **Carga**
- Se insertan datos en un **modelo dimensional** en PostgreSQL con claves forÃ¡neas:
  - Dimensiones: paÃ­s, salud, fumador, exposiciÃ³n, diagnÃ³stico, tratamiento, estadio
  - Tabla de hechos: hechos_persona

### 4. **Streaming**
- Kafka `producer.py` extrae mÃ©tricas agregadas desde PostgreSQL
- Kafka `consumer.py` recibe los datos para validaciÃ³n
- Streamlit visualiza las mÃ©tricas en tiempo real

---

## ğŸ“Š Dashboards

- **Power BI**: anÃ¡lisis exploratorio por paÃ­s, tabaquismo, acceso a salud (ver carpeta `Dashboard/`)
- **Streamlit (tiempo real)**: Ãºltimos valores enviados desde Kafka (aÃ±os fumando, cigarrillos/dÃ­a, tasa de prevalencia y mortalidad)

---

## â–¶ï¸ CÃ³mo ejecutar el proyecto

### 1. Clonar el repositorio

```bash
git clone https://github.com/usuario/etl-lung-cancer.git
cd etl-lung-cancer/Docker
```

### 2. Levantar los servicios con Docker Compose

```bash
docker-compose up --build
```

Este comando iniciarÃ¡:
- PostgreSQL con el esquema creado automÃ¡ticamente
- Airflow Webserver y Scheduler
- Kafka y Zookeeper
- Productor y consumidor Kafka
- Dashboard Streamlit accesible en `http://localhost:8501`
- Airflow accesible en `http://localhost:8080` (usuario: `airflow`, contraseÃ±a: `airflow`)
- Kafka UI accesible en `http://localhost:8089`

### 3. Ejecutar el DAG desde Airflow

1. Ir a `http://localhost:8080`
2. Activar y lanzar el DAG llamado `etl_dimensional_model`

---

## âœ… Requisitos del sistema

- Docker Desktop
- Puertos disponibles: 5433, 8080, 8501, 8089
- Espacio disponible: ~2GB

---

## ğŸ§ª Pruebas Unitarias

Se incluyen pruebas con `pytest` en el directorio `test/`, donde se valida:

- ExtracciÃ³n y filtrado desde la API
- TransformaciÃ³n de nombres de paÃ­s y limpieza de valores

Para correrlas:

```bash
pip install -r requirements.txt
pytest test/
```

---


## ğŸ‘¤ Autor

- **Luis Ãngel GarcÃ­a (2230177)**
- **Antnio Cardenas jurado (2230433)** 
- **Juliana Toro (2225658)** 

---


